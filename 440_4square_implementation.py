# -*- coding: utf-8 -*-
"""440_4square_implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KHdplHWzJzXwZVoRUUG8mwBgSI0oHPEL
"""

import requests
import pandas as pd
import numpy as np
import folium
import matplotlib.cm as cm
import matplotlib.colors as colors
#linked job in that city
# entruprenural program

import requests

# Replace with your actual API key
API_KEY = 'fsq34jULCzz+og3KIHasqw8qmGJEWm7eSjyJhI5Lg1/JcwY='
CLIENT_ID = "2J5UCCZPPORLG0ICMPVEP0JGHTJ2JGCN34TYQ0YLLQUIX0FL"
CLIENT_SECRET = "D3P0311L102QBXBAS3IGCVWIRBDAOMEVOGAZDZSYJMCORHGZ"
BASE_URL = 'https://api.foursquare.com/v3/places/search'.format(CLIENT_ID, CLIENT_SECRET)

def get_nearby_coffee_shops(latitude, longitude, radius, type):

    # Define the request parameters
    params = {
        "query": type,
        "ll": f"{latitude},{longitude}",
        "open_now": "true",
        "radius": radius,
        "sort": "DISTANCE",
        "limit": 50,  # Adjust this limit based on the API's maximum allowed per request
        "offset": 0   # Start with the first page
    }

    headers = {
        "Accept": "application/json",
        "Authorization": API_KEY
    }

    # Make the API request
    response = requests.get(BASE_URL, params=params, headers=headers)

    # Check if the request was successful
    if response.status_code == 200:
        data = response.json()
        results = data.get('results', [])
        coffee_shops = []
        # Extract and print the relevant information


        for venue in results:
            name = venue.get('name', 'No Name')
            lat = venue.get('geocodes', {}).get('main', {}).get('latitude', 'No Latitude')
            lng = venue.get('geocodes', {}).get('main', {}).get('longitude', 'No Longitude')
            zip_code = venue.get('location', {}).get('postcode', 'No Zip Code')

            # Append the data as a dictionary to the list
            coffee_shops.append({
                "name": name,
                "latitude": lat,
                "longitude": lng,
                "zip_code": zip_code
            })
    else:
        print(f"Error: {response.status_code}")
        print(response.text)

    df = pd.DataFrame(coffee_shops)
    return df


latitude = 39.952583  # Replace with the desired latitude
longitude = -75.165222  # Replace with the desired longitude
radius = 10000  # Radius in meters


df_test = get_nearby_coffee_shops(latitude, longitude, radius,"Bars")
print(df_test)

df = get_nearby_coffee_shops(latitude, longitude, radius,"Coffee Shops")
print(df)

df['coordinates'] = df[['latitude', 'longitude']].values.tolist()

# Drop the original 'latitude' and 'longitude' columns
df = df.drop(columns=['latitude', 'longitude'])

print(df)

# How many of this query type is in each of these zipcodes
restaurants_count= df.groupby('zip_code').count()
restaurants_count

from sklearn.cluster import KMeans

df[['latitude', 'longitude']] = pd.DataFrame(df['coordinates'].tolist(), index=df.index)

# Step 2: Run k-means on latitude and longitude
k = 3  # Set the number of clusters you want
kmeans = KMeans(n_clusters=k, random_state=0)
df['cluster'] = kmeans.fit_predict(df[['latitude', 'longitude']])

print(df[['name', 'coordinates', 'cluster']])

#restaurants_count.insert(0, 'Cluster labels', kmeans.labels_)
#restaurants_clustering=restaurants_count[list(restaurants_count.columns[0:2])]
#restaurants_clustering.reset_index(inplace=True)
#restaurants_clustering.sort_values(by='zip_code')


# Insert the cluster labels at the beginning of the DataFrame
#df.insert(0, 'Cluster labels', df['cluster'])

# Create a new DataFrame with only 'Cluster labels' and 'zip_code'
restaurants_clustering = df[['Cluster labels', 'coordinates']]

# Reset the index
restaurants_clustering.reset_index(drop=True, inplace=True)

# Sort by 'zip_code'
restaurants_clustering.sort_values(by='coordinates', inplace=True)

print(restaurants_clustering)

# Merge 'restaurants_clustering' with the original 'df' DataFrame on 'coordinates'
#burgor = pd.merge(restaurants_clustering, df, on='coordinates')

# Group by 'coordinates' and 'Cluster labels' to get the average 'coordinates' for each cluster
#aggregated_df = burgor.groupby(['coordinates', 'Cluster labels']).agg({
    #'zip_code': 'first'  # Keep the first zip code value for each cluster and coordinates
#}).reset_index()

#print(aggregated_df)

# Merge data with clustering results to get representative lat/lon per zip code
merged_df = df.groupby('zip_code').agg({
    'latitude': 'mean',  # Average latitude per zip code
    'longitude': 'mean'  # Average longitude per zip code
}).reset_index()
merged_df = pd.merge(df,restaurants_clustering ,on='zip_code', how='inner')

# Create the map centered around State College
map_clusters = folium.Map(location=[latitude, longitude], zoom_start=12)

# Set color scheme for the clusters
kclusters = len(restaurants_clustering['Cluster labels'].unique())
x = np.arange(kclusters)
ys = [i + x + (i * x) ** 2 for i in range(kclusters)]
colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))
rainbow = [colors.rgb2hex(i) for i in colors_array]

# Add markers to the map
for lat, lon, zip_code, cluster in zip(merged_df['latitude'], merged_df['longitude'], merged_df['zip_code'], merged_df['Cluster labels']):
    label = folium.Popup(f"Zip Code: {zip_code}, Cluster: {cluster}", parse_html=True)
    folium.CircleMarker(
        [lat, lon],
        radius=8,
        popup=label,
        color=rainbow[cluster],
        fill=True,
        fill_color=rainbow[cluster],
        fill_opacity=0.7
    ).add_to(map_clusters)

map_clusters

